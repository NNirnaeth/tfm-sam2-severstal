# EvaluaciÃ³n de SAM2 Base (Sin Fine-tuning) - MÃºltiples Modos

Este script evalÃºa los modelos SAM2 Small y Large base (sin fine-tuning) en el dataset de test de Severstal con **tres modos de evaluaciÃ³n diferentes** para comparar rendimiento realista vs teÃ³rico.

## ğŸ¯ Modos de EvaluaciÃ³n

### 1. **30_gt_points** (Modo Original)
- **DescripciÃ³n**: Usa 30 puntos extraÃ­dos directamente del Ground Truth
- **Uso**: ComparaciÃ³n con evaluaciones anteriores, rendimiento mÃ¡ximo teÃ³rico
- **Realismo**: âŒ No realista (requiere GT perfecto)
- **Rendimiento esperado**: Alto (prompts perfectos)

### 2. **gt_points** (Modo Realista con AnotaciÃ³n)
- **DescripciÃ³n**: Usa 1-3 puntos del GT (centroide + punto interior + punto negativo)
- **Uso**: Simula anotaciÃ³n manual realista
- **Realismo**: âœ… Realista (anotaciÃ³n manual tÃ­pica)
- **Rendimiento esperado**: Medio-Alto

### 3. **auto_prompt** (Modo Completamente AutomÃ¡tico)
- **DescripciÃ³n**: Genera prompts automÃ¡ticamente sin usar GT (grid + boxes)
- **Uso**: EvaluaciÃ³n completamente automÃ¡tica, sin intervenciÃ³n humana
- **Realismo**: âœ… Muy realista (producciÃ³n real)
- **Rendimiento esperado**: Bajo-Medio

## ğŸš€ Uso del Script

### Argumentos Principales

```bash
python evaluate_sam2base_large_small.py [OPCIONES]
```

**Opciones disponibles:**
- `--evaluation_mode`: Modo de evaluaciÃ³n (`auto_prompt`, `gt_points`, `30_gt_points`)
- `--num_gt_points`: NÃºmero de puntos GT (1-3) cuando `evaluation_mode=gt_points`
- `--model_size`: TamaÃ±o del modelo (`small`, `large`, `both`)
- `--results_dir`: Directorio para guardar resultados
- `--num_viz`: NÃºmero de visualizaciones a generar

### Ejemplos de Uso

#### 1. EvaluaciÃ³n Completa (Todos los Modos)
```bash
# Modo 1: 30 puntos GT (mÃ¡ximo teÃ³rico)
python evaluate_sam2base_large_small.py --evaluation_mode 30_gt_points --model_size both

# Modo 2: 3 puntos GT (realista)
python evaluate_sam2base_large_small.py --evaluation_mode gt_points --num_gt_points 3 --model_size both

# Modo 3: Prompts automÃ¡ticos (completamente automÃ¡tico)
python evaluate_sam2base_large_small.py --evaluation_mode auto_prompt --model_size both
```

#### 2. EvaluaciÃ³n RÃ¡pida (Solo Large)
```bash
# Con 3 puntos GT
python evaluate_sam2base_large_small.py --evaluation_mode gt_points --num_gt_points 3 --model_size large --num_viz 10
```

#### 3. ComparaciÃ³n de Puntos GT
```bash
# 1 punto (solo centroide)
python evaluate_sam2base_large_small.py --evaluation_mode gt_points --num_gt_points 1 --model_size large

# 2 puntos (centroide + punto interior)
python evaluate_sam2base_large_small.py --evaluation_mode gt_points --num_gt_points 2 --model_size large

# 3 puntos (centroide + punto interior + punto negativo)
python evaluate_sam2base_large_small.py --evaluation_mode gt_points --num_gt_points 3 --model_size large
```

## ğŸ“Š Estructura de Resultados

Los resultados se guardan en:
```
evaluation_results/sam2_base/sam2base_evaluation_{modo}_{timestamp}/
â”œâ”€â”€ sam2_small_base_no_fine-tuning_{modo}/
â”‚   â”œâ”€â”€ sam2base_sam2_small_base_no_fine-tuning_{modo}_{timestamp}.csv
â”‚   â””â”€â”€ visualizations/
â”œâ”€â”€ sam2_large_base_no_fine-tuning_{modo}/
â”‚   â”œâ”€â”€ sam2base_sam2_large_base_no_fine-tuning_{modo}_{timestamp}.csv
â”‚   â””â”€â”€ visualizations/
â”œâ”€â”€ final_results_all_models.json
â””â”€â”€ evaluation_summary.txt
```

## ğŸ“ˆ MÃ©tricas Incluidas

- **MÃ©tricas de SegmentaciÃ³n**: IoU, Dice, Precision, Recall, F1
- **MÃ©tricas de Umbral**: IoU@50, IoU@75, IoU@90, IoU@95
- **MÃ©tricas Benevolentes**: Benevolent@75, Benevolent@any
- **MÃ©tricas de Rendimiento**: Tiempo de inferencia, imÃ¡genes procesadas
- **Visualizaciones**: AnÃ¡lisis TP/FP/FN para muestras aleatorias

## â±ï¸ Tiempos Estimados

| Modelo | Modo | Tiempo Estimado |
|--------|------|-----------------|
| Small | 30_gt_points | ~2.5 horas |
| Small | gt_points | ~2.5 horas |
| Small | auto_prompt | ~3.0 horas |
| Large | 30_gt_points | ~5.0 horas |
| Large | gt_points | ~5.0 horas |
| Large | auto_prompt | ~6.0 horas |

## ğŸ” InterpretaciÃ³n de Resultados

### ComparaciÃ³n de Modos
1. **30_gt_points**: Rendimiento mÃ¡ximo teÃ³rico con prompts perfectos
2. **gt_points**: Rendimiento realista con anotaciÃ³n manual tÃ­pica
3. **auto_prompt**: Rendimiento completamente automÃ¡tico sin GT

### Diferencias Esperadas
- **30_gt_points > gt_points > auto_prompt** en tÃ©rminos de IoU/F1
- **auto_prompt** es el mÃ¡s realista para aplicaciones de producciÃ³n
- **gt_points** balancea realismo y rendimiento

## ğŸ› ï¸ ConfiguraciÃ³n TÃ©cnica

### HiperparÃ¡metros de Auto-Prompt
- **Grid Spacing**: 128px
- **Box Scales**: [128, 256]px
- **Confidence Threshold**: 0.7
- **NMS IoU Threshold**: 0.65
- **Top-K Filter**: 200

### GeneraciÃ³n de Puntos GT
- **1 punto**: Centroide del defecto
- **2 puntos**: Centroide + punto mÃ¡s interior
- **3 puntos**: Centroide + punto interior + punto negativo

## ğŸ“ Notas Importantes

1. **Reproducibilidad**: Seed fijo (42) para resultados consistentes
2. **Memoria**: Auto-prompt puede usar mÃ¡s memoria por la generaciÃ³n de mÃºltiples prompts
3. **Visualizaciones**: Limitadas para evitar problemas de espacio en disco
4. **GPU**: Requiere CUDA para inferencia eficiente

## ğŸ”„ ComparaciÃ³n con Modelos Fine-tuned

Este script proporciona la **baseline** para comparar con:
- SAM2 fine-tuned en subsets (25, 50, 100, 200 imÃ¡genes)
- SAM2 fine-tuned en dataset completo
- SAM2 con LoRA
- YOLO + SAM2 pipeline

Los resultados base ayudan a cuantificar la mejora del fine-tuning.

